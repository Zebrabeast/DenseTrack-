import os
import re
import torch
import numpy as np
from PIL import Image
from lavis.models import load_model_and_preprocess

# è®¾å¤‡é…ç½®
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# åŠ è½½CLIPæ¨¡å‹
model, vis_processors, txt_processors = load_model_and_preprocess(
    "clip_feature_extractor", model_type="ViT-B-16", is_eval=True, device=device
)

# ä¿å­˜ç‰¹å¾çš„ç»“æ„å®šä¹‰
dtype = np.dtype([
    ('id', np.int32),         # è¡ŒäººID
    ('coordinate', np.float32, 2),  # ä¸­å¿ƒç‚¹åæ ‡(x, y)
    ('feature', np.float32, 512)    # CLIPè¾“å‡ºç‰¹å¾
])

# è¯»å– txt ä¸­çš„åæ ‡
def read_coordinates(txt_file):
    coordinates = []
    with open(txt_file, 'r') as file:
        for line in file:
            count_str, x_str, y_str = line.strip().split(',')
            count = int(count_str)
            x = float(x_str)
            y = float(y_str)
            coordinates.append((count, x, y))
    return coordinates

# ä¸»å¤„ç†å‡½æ•°ï¼šè£å‰ª + æç‰¹å¾ + ä¿å­˜
"""é’ˆå¯¹test_1çš„æ”¹è¿›ï¼Œ1.è¿›è¡Œäº†è¾¹ç•Œåˆ¤æ–­ï¼Œè¶…å‡ºè¾¹ç•Œçš„ç‚¹ï¼Œè®¾ç½®è£å‰ªèŒƒå›´å¤„äºå›¾åƒå†…
                    2.ä»åŸæ¥çš„å¾ªç¯ï¼Œè£å‰ªä¸€ä¸ªï¼Œæå–ä¸€ä¸ªæ”¹ä¸ºè£å‰ªä¸€å¸§ï¼Œæå–ä¸€å¸§ï¼Œåé¢å¯èƒ½è¿˜è¦æ”¹æˆæ•°æ®é›†  """
# def process_and_extract(image_folder, txt_folder, feature_save_root):
#     os.makedirs(feature_save_root, exist_ok=True)

#     image_files = [f for f in os.listdir(image_folder) if f.endswith(('.jpg', '.png', '.jpeg'))]

#     for image_file in image_files:
#         image_prefix = image_file.split(".")[0]
#         txt_file = os.path.join(txt_folder, f"{image_prefix}_fused_positions.txt")
#         if not os.path.exists(txt_file):
#             print(f"âŒ æ‰¾ä¸åˆ°åŒ¹é…çš„åæ ‡æ–‡ä»¶: {txt_file}")
#             continue

#         image_path = os.path.join(image_folder, image_file)
#         img = Image.open(image_path).convert("RGB")
#         coordinates = read_coordinates(txt_file)  # [(pid, x, y), ...]

#         if not coordinates:
#             continue

#         crops, pid_xy_list = [], []
#         for pid, x, y in coordinates:
#             W, H = img.size  # æ³¨æ„ PIL å›¾åƒæ˜¯ (å®½, é«˜)  æ§åˆ¶è£å‰ªçš„èŒƒå›´ï¼Œä½¿å…¶åœ¨ä¹‹ä¸­
#             left = max(0, x - 10)
#             top = max(0, y - 10)
#             right = min(W, x + 10)
#             bottom = min(H, y + 10)
#             cropped_img = img.crop((left, top, right, bottom))
#             crops.append(cropped_img)  #è¿™é‡Œçš„å¯¹åº”ï¼Œä¿è¯æ•°æ®æ˜¯å¯¹åº”çš„ï¼Œåé¢å¦‚æœæœ‰é—®é¢˜ï¼Œå¯ä»¥ä»è¿™é‡ŒæŸ¥çœ‹
#             pid_xy_list.append((pid, x, y))

#         # æ‰¹é‡å›¾åƒå¤„ç†
#         image_tensors = torch.stack([vis_processors["eval"](c) for c in crops]).to(device)
#         text_input = txt_processors["eval"]("A pedestrian")
#         sample = {"image": image_tensors, "text_input": [text_input] * len(crops)}
#         features = model.extract_features(sample)
#         image_embeds = features.image_embeds.cpu().detach().numpy()

#         frame_data = []
#         for (pid, x, y), embed in zip(pid_xy_list, image_embeds):
#             frame_data.append((pid, (x, y), embed))
#             print(f"âœ… æå–ç‰¹å¾: {image_prefix}_{pid}_{x}_{y} | ç»´åº¦: {embed.shape}")

#         if frame_data:
#             structured_data = np.array(frame_data, dtype=dtype)
#             save_path = os.path.join(feature_save_root, f"{image_prefix}.npy")
#             np.save(save_path, structured_data)
#             print(f"ğŸ’¾ å·²ä¿å­˜ç‰¹å¾: {save_path}")

#     print("âœ… å…¨éƒ¨å¤„ç†å®Œæˆï¼")
# def process_and_extract(image_folder, txt_folder, feature_save_root):
#     os.makedirs(feature_save_root, exist_ok=True)

#     image_files = [f for f in os.listdir(image_folder) if f.endswith(('.jpg', '.png', '.jpeg'))]

#     for image_file in image_files:
#         image_prefix = image_file.split(".")[0]
#         txt_file = os.path.join(txt_folder, f"{image_prefix}_fused_positions.txt")
#         if not os.path.exists(txt_file):
#             print(f"âŒ æ‰¾ä¸åˆ°åŒ¹é…çš„åæ ‡æ–‡ä»¶: {txt_file}")
#             continue

#         image_path = os.path.join(image_folder, image_file)
#         img = Image.open(image_path).convert("RGB")
#         coordinates = read_coordinates(txt_file)  # [(pid, x, y), ...]

#         if not coordinates:
#             continue

#         crops, pid_xy_list = [], []
#         for pid, x, y in coordinates:
#             W, H = img.size  # æ³¨æ„ PIL å›¾åƒæ˜¯ (å®½, é«˜)  æ§åˆ¶è£å‰ªçš„èŒƒå›´ï¼Œä½¿å…¶åœ¨ä¹‹ä¸­
#             left = max(0, x - 10)
#             top = max(0, y - 10)
#             right = min(W, x + 10)
#             bottom = min(H, y + 10)
#             cropped_img = img.crop((left, top, right, bottom))
#             crops.append(cropped_img)  #è¿™é‡Œçš„å¯¹åº”ï¼Œä¿è¯æ•°æ®æ˜¯å¯¹åº”çš„ï¼Œåé¢å¦‚æœæœ‰é—®é¢˜ï¼Œå¯ä»¥ä»è¿™é‡ŒæŸ¥çœ‹
#             pid_xy_list.append((pid, x, y))
#         frame_data = []
#         # æ‰¹é‡å›¾åƒå¤„ç† ä¸çŸ¥é“æœ‰æ²¡æœ‰åŒºåˆ«å“ˆ
#         for (pid, x, y), cropped_img in zip(pid_xy_list, crops):
#             image_tensor = vis_processors["eval"](cropped_img).unsqueeze(0).to(device)
#             text_input = txt_processors["eval"]("A pedestrian")
#             sample = {"image": image_tensor, "text_input": [text_input]}
#             feature_image = model.extract_features(sample)
#             image_embed = feature_image.image_embeds.squeeze(0).cpu().detach().numpy()

#            #ä¿å­˜æå–çš„ç‰¹å¾ï¼Œç”±äºå¯èƒ½çš„æ˜¾å­˜è¶…å‡ºé—®é¢˜ï¼Œæš‚æ—¶è¿™ä¹ˆåš
#             frame_data.append((pid, (x, y), image_embed))
#             print(f"âœ… æå–ç‰¹å¾: {image_prefix}_{pid}_{x}_{y} | ç»´åº¦: {image_embed.shape}")

#         if frame_data:
#             structured_data = np.array(frame_data, dtype=dtype)
#             save_path = os.path.join(feature_save_root, f"{image_prefix}.npy")
#             np.save(save_path, structured_data)
#             print(f"ğŸ’¾ å·²ä¿å­˜ç‰¹å¾: {save_path}")

#     print("âœ… å…¨éƒ¨å¤„ç†å®Œæˆï¼")
   #å†æ¬¡æ”¹è¿›ï¼Œä½¿ç”¨batch_sizeæå‡é€Ÿåº¦ï¼Œé‰´äºæ˜¾å­˜å ç”¨å°‘ï¼ŒGPUå ç”¨ä¹Ÿå°‘

def process_and_extract(image_folder, txt_folder, feature_save_root, batch_size= 16):
    os.makedirs(feature_save_root, exist_ok=True)

    image_files = [f for f in os.listdir(image_folder) if f.endswith(('.jpg', '.png', '.jpeg'))]

    for image_file in image_files:
        image_prefix = image_file.split(".")[0]
        txt_file = os.path.join(txt_folder, f"{image_prefix}_fused_positions.txt")
        #è¿™é‡Œæ˜¯é’ˆå¯¹å·²å­˜åœ¨çš„è¿‡æ»¤ï¼Œä¾¿äºç»ˆç«¯æ¢å¤
        save_path = os.path.join(feature_save_root, f"{image_prefix}.npy")
        # âœ… æ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨æå–å¥½çš„ç‰¹å¾
        if os.path.exists(save_path):
            print(f"â­ï¸ å·²å­˜åœ¨ç‰¹å¾æ–‡ä»¶ï¼Œè·³è¿‡: {save_path}")
            continue
        
        if not os.path.exists(txt_file):
            print(f"âŒ æ‰¾ä¸åˆ°åŒ¹é…çš„åæ ‡æ–‡ä»¶: {txt_file}")
            continue

        image_path = os.path.join(image_folder, image_file)
        img = Image.open(image_path).convert("RGB")
        coordinates = read_coordinates(txt_file)  # [(pid, x, y), ...]

        if not coordinates:
            continue

        # æ”¶é›†è£å‰ªå›¾åƒå’Œå¯¹åº”çš„ pid, x, y
        crops, pid_xy_list = [], []
        for pid, x, y in coordinates:
            W, H = img.size  # æ³¨æ„ PIL å›¾åƒæ˜¯ (å®½, é«˜)
            left = max(0, x - 10)
            top = max(0, y - 10)
            right = min(W, x + 10)
            bottom = min(H, y + 10)
            cropped_img = img.crop((left, top, right, bottom))
            crops.append(cropped_img)
            pid_xy_list.append((pid, x, y))

        # æ‰¹é‡å¤„ç†å›¾åƒ
        frame_data = []
        for i in range(0, len(crops), batch_size):
            batch_crops = crops[i:i + batch_size]
            batch_pid_xy_list = pid_xy_list[i:i + batch_size]

            # è½¬æ¢ä¸º tensor å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
            image_tensors = torch.stack([vis_processors["eval"](c) for c in batch_crops]).to(device)
            text_input = txt_processors["eval"]("A pedestrian")
            sample = {"image": image_tensors, "text_input": [text_input] * len(batch_crops)}

            features = model.extract_features(sample)
            image_embeds = features.image_embeds.cpu().detach().numpy()

            for (pid, x, y), embed in zip(batch_pid_xy_list, image_embeds):
                frame_data.append((pid, (x, y), embed))
                print(f"âœ… æå–ç‰¹å¾: {image_prefix}_{pid}_{x}_{y} | ç»´åº¦: {embed.shape}")

        # ä¿å­˜ä¸º npy æ–‡ä»¶
        if frame_data:
            structured_data = np.array(frame_data, dtype=dtype)
            save_path = os.path.join(feature_save_root, f"{image_prefix}.npy")
            np.save(save_path, structured_data)
            print(f"ğŸ’¾ å·²ä¿å­˜ç‰¹å¾: {save_path}")

    print("âœ… å…¨éƒ¨å¤„ç†å®Œæˆï¼")


# #  åŸå›¾åœ°å€
# image_folder = '/home/data_SSD/zk/dataset/00011/origin'
# # èåˆåæ ‡åœ°å€
# txt_folder = '/home/data_SSD/zk/dataset/00011/fused_txt'

# feature_save_root = '/home/data_SSD/zk/dataset/00011/features'

# process_and_extract(image_folder, txt_folder, feature_save_root)



root_dir = '/home/data_SSD/zk/dataset/special_case'

sequence_dirs = sorted([
    os.path.join(root_dir, d) for d in os.listdir(root_dir)
    if os.path.isdir(os.path.join(root_dir, d))
])
#è¿™é‡Œåˆ°00110åºåˆ—å·
for seq_dir in sequence_dirs:
    seq_name = os.path.basename(seq_dir)
    txt_folder =  os.path.join(seq_dir,"fused_txt")
    feature_save_root = os.path.join(seq_dir, "features") #è¾“å‡º ç‰¹å¾è¿›è¡Œä¿å­˜
    os.makedirs(feature_save_root, exist_ok=True)
    #å¯¹åº”çš„ åŸå§‹å›¾åƒä½ç½®
    image_folder = os.path.join(seq_dir, "origin")
    print(f"\n==> Start processing sequence: {seq_name}")
    try:
        process_and_extract(image_folder, txt_folder, feature_save_root)
    except Exception as e:
        print(f"[ERROR] Failed to process {seq_name}: {e}")
    torch.cuda.empty_cache()  # æ¸…ç†æ˜¾å­˜