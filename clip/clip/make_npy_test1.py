import os
import re
import torch
import numpy as np
from PIL import Image
from lavis.models import load_model_and_preprocess

# è®¾å¤‡é…ç½®
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# åŠ è½½CLIPæ¨¡å‹
model, vis_processors, txt_processors = load_model_and_preprocess(
    "clip_feature_extractor", model_type="ViT-B-16", is_eval=True, device=device
)

# ä¿å­˜ç‰¹å¾çš„ç»“æ„å®šä¹‰
dtype = np.dtype([
    ('id', np.int32),         # è¡ŒäººID
    ('coordinate', np.float32, 2),  # ä¸­å¿ƒç‚¹åæ ‡(x, y)
    ('feature', np.float32, 512)    # CLIPè¾“å‡ºç‰¹å¾
])

# è¯»å– txt ä¸­çš„åæ ‡
def read_coordinates(txt_file):
    coordinates = []
    with open(txt_file, 'r') as file:
        for line in file:
            count_str, x_str, y_str = line.strip().split(',')
            count = int(count_str)
            x = float(x_str)
            y = float(y_str)
            coordinates.append((count, x, y))
    return coordinates

# ä¸»å¤„ç†å‡½æ•°ï¼šè£å‰ª + æç‰¹å¾ + ä¿å­˜
"""é’ˆå¯¹testçš„æ”¹è¿›ï¼Œ1.å°†è£å‰ªéƒ¨åˆ†é›†æˆåˆ°æå–éƒ¨åˆ†äº†ï¼Œçœäº†ä¿å­˜è£å‰ªå›¾åƒçš„æ­¥éª¤ï¼Œ
                     æš‚æ—¶è®¾å®šä¸ºè£å‡ºä¸€å¼ æ£€æŸ¥ä¸€å¼ """
def process_and_extract(image_folder, txt_folder, feature_save_root):
    os.makedirs(feature_save_root, exist_ok=True)

    image_files = [f for f in os.listdir(image_folder) if f.endswith(('.jpg', '.png', '.jpeg'))]

    for image_file in image_files:
        image_prefix = image_file.split(".")[0]
        # if image_prefix != "img110300":   ä¹‹å‰æ¼ä¸‹çš„éƒ¨åˆ†ï¼ˆå½“æ—¶ä¸ºäº†æ£€æŸ¥è¾“å‡ºæ˜¯å¦åˆç†ï¼‰
        #     continue
        txt_file = os.path.join(txt_folder, f"{image_prefix}_fused_positions.txt")
        if not os.path.exists(txt_file):
            print(f"âŒ æ‰¾ä¸åˆ°åŒ¹é…çš„åæ ‡æ–‡ä»¶: {txt_file}")
            continue

        # è¯»å–å›¾åƒå’Œåæ ‡
        image_path = os.path.join(image_folder, image_file)
        img = Image.open(image_path).convert("RGB")
        coordinates = read_coordinates(txt_file)

        frame_data = []

        for pid, x, y in coordinates:
            left, top, right, bottom = x - 10, y - 10, x + 10, y + 10
            cropped_img = img.crop((left, top, right, bottom))

            # ç‰¹å¾æå–
            image_tensor = vis_processors["eval"](cropped_img).unsqueeze(0).to(device)
            text_input = txt_processors["eval"]("A pedestrian")
            sample = {"image": image_tensor, "text_input": [text_input]}
            feature_image = model.extract_features(sample)
            image_embed = feature_image.image_embeds.squeeze(0).cpu().detach().numpy()

            # å­˜å‚¨ä¸ºç»“æ„åŒ–æ•°æ®
            frame_data.append((pid, (x, y), image_embed))
            print(f"âœ… æå–ç‰¹å¾: {image_prefix}_{pid}_{x}_{y} | ç»´åº¦: {image_embed.shape}")

        # ä¿å­˜ä¸ºnpy
        if frame_data:
            structured_data = np.array(frame_data, dtype=dtype)
            save_path = os.path.join(feature_save_root, f"{image_prefix}.npy")
            np.save(save_path, structured_data)
            print(f"ğŸ’¾ å·²ä¿å­˜ç‰¹å¾: {save_path}")

    print("âœ… å…¨éƒ¨å¤„ç†å®Œæˆï¼")

#  åŸå›¾åœ°å€
image_folder = '/home/data_HDD/zk/dataset/00011/origin'
# èåˆåæ ‡åœ°å€
txt_folder = '/home/data_HDD/zk/dataset/00011/fused_txt'

feature_save_root = '/home/data_HDD/zk/dataset/00011/features'

process_and_extract(image_folder, txt_folder, feature_save_root)